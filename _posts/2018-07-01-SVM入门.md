---
layout: post
title: 'SVM入门'
date: 2018-07-01
author: Buer
color: rgb(255,210,32)
cover: 'http://qiniu.bueryo.com/bueryo/20180620221702.jpeg'
tags: svm 机器学习 支持向量机
---
# SVM 支持向量机
## 概念
支持向量机（Support Vector Machine， 常简称为SVM），属于监督学习，可以广泛应用于统计分类以及回归分析。其核心思想是将向量映射到更高维的空间里，在这个空间里建立个最大间隔的超平面。再分开数据的超平面的两边建立两个相互平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面的距离或差距越大，分类器的总误差越小。

## 线性可分的情况

![最优超平面](https://pic1.zhimg.com/v2-771b791fa4ff9488e8f16765c8b4dfad_b.jpg )  

图中c的边距（Margin）最大，相对于a和b，c的泛化性更强。

![SVM有时得到的最优解并不好](https://pic3.zhimg.com/v2-8e76092ef4a1d38a4cbdd85b73a92aa9_b.jpg)

根据边距最大化的原则，B是SVM算法得到的最优超平面，但是从实际上来看，A才能够将两类完全区分开。

![线性不可分下的最优超平面](https://pic4.zhimg.com/v2-287dd571a4de717667c89285d3d4b09b_b.jpg)

如上图，这两类无法用一条直线去划分，基于SVM的边距最大化原则，我们得到的最优超平面只能是这样的：

![](https://pic4.zhimg.com/v2-b30e4e718c1b552d8efe197c6de428f3_b.jpg)

可以看到有时候SVM不能完全将两类很好地分开，但是我们仍然觉得它是比较好的分类器，这是因为我们训练的样本中有些数据本来就是噪声，存在异常值，如果我们在训练（学习）的时候把这些错误的点学习到了，那么模型在下次碰到这些错误情况的时候就难免出错了。这种学习的时候学到了“噪声”的过程就是一个过拟合（over-fitting），这在机器学习中是一个大忌，而SVM在拟合时会保证一定的容错性，忽略异常值而考虑全局性的类别分布。


## 非线性可分的情况

![](https://pic3.zhimg.com/v2-762eece3fbeacca42792c5e1ee4cf084_b.jpg)  

如上图，两个类之间不能有线性超平面，那么SVM如何对这两个类进行分类？之前我们构建分类函数为f(x) = w.x + b（w.x表示w与x的内积），这里就需要我们可以让空间从原本的线性空间变成一个更高维的空间，在这个高维的线性空间下，再用一个超平面进行划分。

![](https://pic4.zhimg.com/v2-88ea3cc8268102f75554b27a347b1501_b.jpg)  

我们构建新特征$z=x^2+y^2$,然后在x、z轴上绘制样本点，可以看到如何去有效分类这两种类别。这就是增加了一个新的维度，在高维空间划分两类。SVM有一种称为核函数的技术。这些函数采用低维输入空间并将其转换为更高维空间，即将不可分离问题转换为可分离问题，这些函数称为内核。它主要用于非线性分离问题。  

![](https://pic3.zhimg.com/v2-9e6a83c3bb4f9932523b8424489e09fb_b.jpg)

## 总结
- 边距最大化
- 忽略异常值
- 使用核函数处理非线性的情况




图片来源：知乎https://zhuanlan.zhihu.com/p/34367375
